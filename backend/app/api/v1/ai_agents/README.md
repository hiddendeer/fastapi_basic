# LangChain 1.2.6 æ ¸å¿ƒæŠ€æœ¯æŒ‡å—

> åŸºäº LangChain 1.2.6+ ç‰ˆæœ¬çš„å®æˆ˜æ•™å­¦ï¼Œå±•ç¤ºæœ€å¸¸ç”¨ã€æœ€å®ç”¨çš„ AI åº”ç”¨å¼€å‘æŠ€æœ¯ã€‚

---

## ğŸ“š ç›®å½•

- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [API æ¥å£è¯¦è§£](#api-æ¥å£è¯¦è§£)
- [å…³é”®æŠ€æœ¯ç‚¹](#å…³é”®æŠ€æœ¯ç‚¹)
- [å¸¸è§é™·é˜±](#å¸¸è§é™·é˜±)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1. LCEL (LangChain Expression Language)

LangChain 1.0+ çš„æ ¸å¿ƒç¼–ç¨‹èŒƒå¼ï¼Œä½¿ç”¨ç®¡é“ç¬¦ `|` æ„å»ºé“¾å¼è°ƒç”¨ï¼š

```python
chain = prompt | llm | parser
result = await chain.ainvoke({"question": "ç”¨æˆ·é—®é¢˜"})
```

**ä¼˜åŠ¿**ï¼š
- ä»£ç ç®€æ´ç›´è§‚
- è‡ªåŠ¨ä¼˜åŒ–æ‰§è¡Œæµç¨‹
- æ”¯æŒæµå¼è¾“å‡º
- æ˜“äºè°ƒè¯•å’Œç»´æŠ¤

### 2. LangGraph

LangChain 1.2+ å¼•å…¥çš„æ–°æ¶æ„ï¼Œç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„ AI åº”ç”¨ï¼š

- **Agent**: åŸºäº LangGraph çš„æ™ºèƒ½ä»£ç†
- **State**: çŠ¶æ€ç®¡ç†ï¼Œæ”¯æŒå¤šè½®å¯¹è¯
- **Checkpointer**: æŒä¹…åŒ–çŠ¶æ€ï¼ˆæ”¯æŒ Redis/æ•°æ®åº“ï¼‰

### 3. æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | ä½œç”¨ | å¸¸ç”¨ç±» |
|------|------|--------|
| **LLM** | å¤§è¯­è¨€æ¨¡å‹æ¥å£ | `ChatOpenAI` |
| **Prompt** | æç¤ºè¯æ¨¡æ¿ | `ChatPromptTemplate` |
| **Parser** | è¾“å‡ºè§£æå™¨ | `StrOutputParser`, `PydanticOutputParser` |
| **History** | å¯¹è¯å†å²ç®¡ç† | `RunnableWithMessageHistory` |
| **Agent** | å·¥å…·è°ƒç”¨ä»£ç† | `create_agent` (LangGraph) |

---

## ç¯å¢ƒé…ç½®

### 1. å®‰è£…ä¾èµ–

```bash
# backend/pyproject.toml
langchain>=1.2.6
langchain-openai>=1.1.7
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

åœ¨é¡¹ç›®æ ¹ç›®å½• `.env` æ–‡ä»¶ä¸­ï¼š

```env
# LLM é…ç½®
LLM_MODEL_ID=gpt-4o
LLM_API_KEY=your_api_key
LLM_BASE_URL=https://api.openai.com/v1
```

### 3. å¯åŠ¨é¡¹ç›®

```bash
cd backend
uv run fastapi dev app/main.py
```

è®¿é—® Swagger UI: `http://localhost:8000/docs`

---

## API æ¥å£è¯¦è§£

### 1. åŸºç¡€å¯¹è¯ `/chat`

**åŠŸèƒ½**ï¼šæœ€ç®€å•çš„ LangChain ç”¨æ³•æ¼”ç¤º

```python
@router.post("/chat", response_model=ChatResponse)
async def basic_chat(request: ChatRequest):
    llm = get_llm()

    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜"),
        ("user", "{question}")
    ])

    # LCEL é“¾å¼è°ƒç”¨
    chain = prompt | llm | StrOutputParser()
    result = await chain.ainvoke({"question": request.message})

    return ChatResponse(response=result)
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `from_messages()` åˆ›å»ºç»“æ„åŒ–æç¤ºè¯
- `StrOutputParser` è‡ªåŠ¨æå–æ–‡æœ¬å†…å®¹
- `ainvoke()` å¼‚æ­¥è°ƒç”¨ï¼ˆæ¨èï¼‰

---

### 2. æµå¼å“åº” `/stream`

**åŠŸèƒ½**ï¼šå®æ—¶æ¨é€ LLM ç”Ÿæˆè¿‡ç¨‹

```python
@router.post("/stream")
async def streaming_chat(request: ChatRequest):
    llm = get_llm(streaming=True)
    prompt = ChatPromptTemplate.from_template("{question}")
    chain = prompt | llm | StrOutputParser()

    async def event_generator():
        async for chunk in chain.astream({"question": request.message}):
            yield chunk

    return StreamingResponse(event_generator(), media_type="text/event-stream")
```

**å…³é”®ç‚¹**ï¼š
- `streaming=True` å¯ç”¨æµå¼è¾“å‡º
- `astream()` è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨
- FastAPI çš„ `StreamingResponse` è‡ªåŠ¨å¤„ç† SSE

---

### 3. ç¿»è¯‘åŠŸèƒ½ `/translate`

**åŠŸèƒ½**ï¼šLCEL é“¾å¼è°ƒç”¨ + æµå¼è¾“å‡º

```python
@router.post("/translate")
async def translate_text(request: TranslationRequest):
    llm = get_llm(streaming=True)

    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä½ç²¾é€šå¤šç§è¯­è¨€çš„ç¿»è¯‘å®˜ã€‚"),
        ("user", "è¯·å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆ{target_language}ï¼š\n\n{text}")
    ])

    chain = prompt | llm | StrOutputParser()

    async def event_generator():
        async for chunk in chain.astream({
            "text": request.text,
            "target_language": request.target_language
        }):
            yield chunk

    return StreamingResponse(event_generator(), media_type="text/event-stream")
```

**å…³é”®ç‚¹**ï¼š
- æ”¯æŒå¤šå‚æ•°ä¼ é€’
- æµå¼è¿”å›ç¿»è¯‘ç»“æœ
- æç¤ºè¯æ¨¡æ¿ä¸­çš„å˜é‡è‡ªåŠ¨æ›¿æ¢

---

### 4. ç»“æ„åŒ–è¾“å‡º `/extract`

**åŠŸèƒ½**ï¼šå¼ºåˆ¶ LLM è¿”å› Pydantic æ¨¡å‹æ•°æ®

```python
@router.post("/extract", response_model=PersonInfo)
async def extract_info(request: ExtractionRequest):
    llm = get_llm()

    # åˆå§‹åŒ–è§£æå™¨
    parser = PydanticOutputParser(pydantic_object=PersonInfo)

    # æ³¨å…¥æ ¼å¼åŒ–æŒ‡ä»¤
    prompt = ChatPromptTemplate.from_template(
        "ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–äººç‰©ä¿¡æ¯ã€‚\n{format_instructions}\næ–‡æœ¬å†…å®¹ï¼š{text}"
    )
    prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    chain = prompt | llm | parser
    return await chain.ainvoke({"text": request.text})
```

**å…³é”®ç‚¹**ï¼š
- `PydanticOutputParser` ç¡®ä¿è¾“å‡ºæ ¼å¼
- `partial()` é¢„å¡«å……éƒ¨åˆ†å˜é‡
- è‡ªåŠ¨è§£æä¸º Pydantic å¯¹è±¡

---

### 5. æ–‡ä»¶ç¿»è¯‘ `/translate-file`

**åŠŸèƒ½**ï¼šæ”¯æŒ PDF/Word æ–‡ä»¶ä¸Šä¼ å¹¶ç¿»è¯‘

```python
@router.post("/translate-file")
async def translate_file(
    file: UploadFile = File(...),
    target_language: str = Form("zh-CN")
):
    content = await file.read()
    text = ""

    # PDF å¤„ç†
    if file.filename.lower().endswith(".pdf"):
        pdf_reader = pypdf.PdfReader(io.BytesIO(content))
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"

    # DOCX å¤„ç†
    elif file.filename.lower().endswith(".docx"):
        doc = docx.Document(io.BytesIO(content))
        for para in doc.paragraphs:
            text += para.text + "\n"

    # ä½¿ç”¨ LLM ç¿»è¯‘
    llm = get_llm(streaming=True)
    chain = prompt | llm | StrOutputParser()

    async def event_generator():
        async for chunk in chain.astream({
            "text": text,
            "target_language": target_language
        }):
            yield chunk

    return StreamingResponse(event_generator(), media_type="text/event-stream")
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `pypdf` æå– PDF æ–‡æœ¬
- ä½¿ç”¨ `python-docx` å¤„ç† Word æ–‡æ¡£
- æµå¼è¿”å›ç¿»è¯‘ç»“æœ

---

### 6. å¸¦è®°å¿†çš„å¯¹è¯ `/langchain-function`

**åŠŸèƒ½**ï¼šå¤šè½®å¯¹è¯è®°å¿†ç®¡ç†

```python
@router.post("/langchain-function")
async def langchainFunction(request: ChatRequest):
    llm = get_llm()

    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œè¯·æ ¹æ®å¯¹è¯å†å²å›ç­”ç”¨æˆ·çš„é—®é¢˜"),
        ("placeholder", "{chat_history}"),  # å ä½ç¬¦ï¼šå†å²è®°å½•
        ("user", "{input}")  # âš ï¸ æ³¨æ„ï¼šä¸è¦ç”¨ f-string
    ])

    chain = prompt | llm | StrOutputParser()

    # å¯¹è¯å†å²å­˜å‚¨
    store: Dict[str, list] = {}

    def get_session_history(session_id: str) -> BaseChatMessageHistory:
        if session_id not in store:
            store[session_id] = []
        history = InMemoryChatMessageHistory()
        for msg in store.get(session_id, []):
            history.add_message(msg)
        return history

    # åŒ…è£…é“¾ä»¥æ”¯æŒå†å²è®°å½•
    conversational_chain = RunnableWithMessageHistory(
        runnable=chain,
        get_session_history=get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
    )

    config = {"configurable": {"session_id": "default_session"}}
    result = await conversational_chain.ainvoke(
        {"input": request.message},
        config=config
    )

    return result
```

**å…³é”®ç‚¹**ï¼š
- `placeholder` ç”¨äºæ³¨å…¥å†å²è®°å½•
- `session_id` åŒºåˆ†ä¸åŒä¼šè¯
- ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis/æ•°æ®åº“å­˜å‚¨å†å²

---

### 7. Agent å·¥å…·è°ƒç”¨ `/langchainReact`

**åŠŸèƒ½**ï¼šLangChain 1.2+ æ–° APIï¼ŒåŸºäº LangGraph

```python
async def langchainReact(request: ChatRequest):
    from langchain.agents import create_agent

    llm = get_llm()

    # å®šä¹‰å·¥å…·
    tools = []  # æ·»åŠ ä½ çš„å·¥å…·

    # åˆ›å»º Agent (LangGraph æ–¹å¼)
    agent = create_agent(
        model=llm,
        tools=tools,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"
    )

    # è°ƒç”¨ Agent
    result = await agent.ainvoke({
        "messages": [("user", request.message)]
    })

    # è¿”å›æœ€åä¸€æ¡æ¶ˆæ¯
    return result["messages"][-1].content
```

**å…³é”®ç‚¹**ï¼š
- LangChain 1.2+ ä½¿ç”¨ `create_agent` (LangGraph)
- ä¸å†éœ€è¦ `AgentExecutor`
- è¾“å…¥æ ¼å¼ï¼š`{"messages": [("user", "æ–‡æœ¬")]}`
- è¾“å‡ºæ ¼å¼ï¼š`{"messages": [...]}`

---

## å…³é”®æŠ€æœ¯ç‚¹

### 1. æç¤ºè¯æ¨¡æ¿çš„ä¸‰ç§å†™æ³•

#### å†™æ³•ä¸€ï¼šç®€å•æ¨¡æ¿
```python
prompt = ChatPromptTemplate.from_template("å›ç­”é—®é¢˜ï¼š{question}")
```

#### å†™æ³•äºŒï¼šæ¶ˆæ¯åˆ—è¡¨ï¼ˆæ¨èï¼‰
```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"),
    ("user", "{question}")
])
```

#### å†™æ³•ä¸‰ï¼šéƒ¨åˆ†é¢„å¡«å……
```python
prompt = ChatPromptTemplate.from_template(
    "{format_instructions}\næ–‡æœ¬ï¼š{text}"
)
prompt = prompt.partial(
    format_instructions="è¯·è¾“å‡º JSON æ ¼å¼"
)
```

### 2. æµå¼ vs éæµå¼

| æ–¹æ³• | ç”¨é€” | è¿”å›å€¼ |
|------|------|--------|
| `invoke()` | åŒæ­¥è°ƒç”¨ | å®Œæ•´ç»“æœ |
| `ainvoke()` | å¼‚æ­¥è°ƒç”¨ï¼ˆæ¨èï¼‰ | å®Œæ•´ç»“æœ |
| `stream()` | åŒæ­¥æµå¼ | ç”Ÿæˆå™¨ |
| `astream()` | å¼‚æ­¥æµå¼ï¼ˆæ¨èï¼‰ | å¼‚æ­¥ç”Ÿæˆå™¨ |

### 3. è¾“å‡ºè§£æå™¨å¯¹æ¯”

| è§£æå™¨ | ç”¨é€” | è¿”å›ç±»å‹ |
|--------|------|----------|
| `StrOutputParser` | çº¯æ–‡æœ¬è¾“å‡º | `str` |
| `PydanticOutputParser` | ç»“æ„åŒ–æ•°æ® | Pydantic æ¨¡å‹ |

---

## å¸¸è§é™·é˜±

### âš ï¸ é™·é˜± 1ï¼šf-string é”™è¯¯ä½¿ç”¨

**é”™è¯¯ä»£ç **ï¼š
```python
("user", f"{input}")  # âŒ ä¼šè¢«æ±‚å€¼ä¸º "<built-in function input>"
```

**æ­£ç¡®å†™æ³•**ï¼š
```python
("user", "{input}")  # âœ… ä½œä¸ºå ä½ç¬¦ï¼Œè¿è¡Œæ—¶æ›¿æ¢
```

### âš ï¸ é™·é˜± 2ï¼šLangChain 1.2 çš„ Agent API å˜åŒ–

**æ—§ API (å·²åºŸå¼ƒ)**ï¼š
```python
from langchain.agents import create_react_agent, AgentExecutor

agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)
executor = AgentExecutor(agent=agent, tools=tools)
result = await executor.ainvoke({"input": message})
```

**æ–° API (LangChain 1.2+)**ï¼š
```python
from langchain.agents import create_agent

agent = create_agent(model=llm, tools=tools, system_prompt=prompt)
result = await agent.ainvoke({"messages": [("user", message)]})
```

### âš ï¸ é™·é˜± 3ï¼šå¯¹è¯å†å²ç±»å‹é”™è¯¯

**é”™è¯¯ä»£ç **ï¼š
```python
store: Dict[str, list[BaseChatMessageHistory]] = {}  # âŒ ç±»å‹é”™è¯¯
```

**æ­£ç¡®å†™æ³•**ï¼š
```python
from langchain_core.messages import BaseMessage
store: Dict[str, list[BaseMessage]] = {}  # âœ… å­˜å‚¨æ¶ˆæ¯åˆ—è¡¨
```

---

## æœ€ä½³å®è·µ

### 1. ä½¿ç”¨å¼‚æ­¥æ–¹æ³•

```python
# âœ… æ¨è
result = await chain.ainvoke({"question": "é—®é¢˜"})

# âŒ é¿å…
result = chain.invoke({"question": "é—®é¢˜"})
```

### 2. ç”Ÿäº§ç¯å¢ƒçŠ¶æ€ç®¡ç†

```python
# å¼€å‘ç¯å¢ƒï¼šå†…å­˜å­˜å‚¨
store = {}

# ç”Ÿäº§ç¯å¢ƒï¼šRedis/æ•°æ®åº“
from redis import Redis
redis_client = Redis(host='localhost', port=6379, db=0)
```

### 3. é”™è¯¯å¤„ç†

```python
try:
    result = await chain.ainvoke({"question": message})
except Exception as e:
    raise HTTPException(status_code=500, detail=f"LLM è°ƒç”¨å¤±è´¥: {str(e)}")
```

### 4. ç›‘æ§å’Œè°ƒè¯•

é›†æˆ [LangSmith](https://smith.langchain.com/) è¿›è¡Œé“¾è·¯è¿½è¸ªï¼š

```python
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your_langsmith_key"
```

---

## é™„å½•ï¼šFastAPI + Redis æ ¸å¿ƒç”¨æ³•

> åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½¿ç”¨ Redis å­˜å‚¨å¯¹è¯å†å²ã€ç¼“å­˜æ•°æ®æ˜¯æœ€å¸¸è§çš„åšæ³•ã€‚ä»¥ä¸‹æ˜¯æ ¸å¿ƒç”¨æ³•æ€»ç»“ã€‚

### 1. å®‰è£…ä¾èµ–

```bash
# backend/pyproject.toml
redis>=5.0.0
```

### 2. åŸºç¡€é…ç½®

```python
# app/core/redis.py
import redis
from typing import Optional
from contextlib import contextmanager

class RedisClient:
    def __init__(self, host: str = "localhost", port: int = 6379, db: int = 0):
        self.client = redis.Redis(
            host=host,
            port=port,
            db=db,
            decode_responses=True  # è‡ªåŠ¨è§£ç ä¸ºå­—ç¬¦ä¸²
        )

    @contextmanager
    def get_connection(self):
        """è·å– Redis è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
        yield self.client

# å…¨å±€å®ä¾‹
redis_client = RedisClient(
    host=os.getenv("REDIS_HOST", "localhost"),
    port=int(os.getenv("REDIS_PORT", 6379)),
    db=int(os.getenv("REDIS_DB", 0))
)
```

### 3. æ ¸å¿ƒæ“ä½œ

#### 3.1 å­—ç¬¦ä¸²æ“ä½œ

```python
from app.core.redis import redis_client

# è®¾ç½®å€¼ï¼ˆæ°¸ä¹…ï¼‰
redis_client.client.set("key", "value")

# è®¾ç½®å€¼ï¼ˆå¸¦è¿‡æœŸæ—¶é—´ï¼Œå•ä½ï¼šç§’ï¼‰
redis_client.client.setex("key", 3600, "value")  # 1å°æ—¶åè¿‡æœŸ

# è·å–å€¼
value = redis_client.client.get("key")

# åˆ é™¤
redis_client.client.delete("key")

# æ£€æŸ¥æ˜¯å¦å­˜åœ¨
exists = redis_client.client.exists("key")
```

#### 3.2 å“ˆå¸Œè¡¨æ“ä½œï¼ˆé€‚åˆå­˜å‚¨ç»“æ„åŒ–æ•°æ®ï¼‰

```python
# è®¾ç½®å“ˆå¸Œå­—æ®µ
redis_client.client.hset("user:123", mapping={
    "name": "å¼ ä¸‰",
    "email": "zhang@example.com",
    "age": "25"
})

# è·å–å•ä¸ªå­—æ®µ
name = redis_client.client.hget("user:123", "name")

# è·å–æ‰€æœ‰å­—æ®µ
user_data = redis_client.client.hgetall("user:123")

# åˆ é™¤å“ˆå¸Œå­—æ®µ
redis_client.client.hdel("user:123", "email")

# è·å–æ‰€æœ‰å­—æ®µå
fields = redis_client.client.hkeys("user:123")
```

#### 3.3 åˆ—è¡¨æ“ä½œï¼ˆé€‚åˆå­˜å‚¨å†å²è®°å½•ï¼‰

```python
# å·¦ä¾§æ’å…¥ï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
redis_client.client.lpush("chat:history:user123", '{"role": "user", "content": "ä½ å¥½"}')
redis_client.client.lpush("chat:history:user123", '{"role": "assistant", "content": "ä½ å¥½ï¼"}')

# è·å–åˆ—è¡¨ï¼ˆæŒ‡å®šèŒƒå›´ï¼‰
history = redis_client.client.lrange("chat:history:user123", 0, 9)  # è·å–å‰10æ¡

# è·å–åˆ—è¡¨é•¿åº¦
length = redis_client.client.llen("chat:history:user123")

# ä¿ç•™æŒ‡å®šèŒƒå›´ï¼ˆåˆ é™¤å…¶ä»–ï¼‰
redis_client.client.ltrim("chat:history:user123", 0, 19)  # åªä¿ç•™å‰20æ¡

# åˆ é™¤åˆ—è¡¨
redis_client.client.delete("chat:history:user123")
```

#### 3.4 JSON æ“ä½œï¼ˆéœ€è¦ RedisJSON æ¨¡å—ï¼‰

```python
import json

# å­˜å‚¨ JSONï¼ˆå­—ç¬¦ä¸²æ–¹å¼ï¼‰
data = {"name": "å¼ ä¸‰", "age": 25}
redis_client.client.set("user:123", json.dumps(data))

# è¯»å– JSON
value = redis_client.client.get("user:123")
user_data = json.loads(value) if value else None
```

### 4. å®æˆ˜ï¼šå¯¹è¯å†å²å­˜å‚¨

#### æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨åˆ—è¡¨å­˜å‚¨

```python
from app.core.redis import redis_client
import json

async def save_chat_message(session_id: str, role: str, content: str):
    """ä¿å­˜å•æ¡æ¶ˆæ¯"""
    key = f"chat:history:{session_id}"
    message = {"role": role, "content": content, "timestamp": time.time()}

    # æ·»åŠ åˆ°åˆ—è¡¨å¤´éƒ¨
    redis_client.client.lpush(key, json.dumps(message))

    # åªä¿ç•™æœ€è¿‘ 100 æ¡
    redis_client.client.ltrim(key, 0, 99)

    # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆ7å¤©ï¼‰
    redis_client.client.expire(key, 604800)

async def get_chat_history(session_id: str, limit: int = 20) -> list:
    """è·å–å†å²è®°å½•"""
    key = f"chat:history:{session_id}"

    # è·å–æœ€è¿‘çš„ N æ¡
    messages = redis_client.client.lrange(key, 0, limit - 1)

    # åè½¬ï¼ˆå› ä¸º lpush æ˜¯å€’åºå­˜å‚¨çš„ï¼‰
    messages.reverse()

    return [json.loads(msg) for msg in messages]

async def clear_chat_history(session_id: str):
    """æ¸…ç©ºå†å²è®°å½•"""
    key = f"chat:history:{session_id}"
    redis_client.client.delete(key)
```

#### æ–¹æ¡ˆäºŒï¼šé›†æˆåˆ° LangChain

```python
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.chat_history import BaseChatMessageHistory
from app.core.redis import redis_client
import json

class RedisChatMessageHistory(BaseChatMessageHistory):
    """åŸºäº Redis çš„å¯¹è¯å†å²ç®¡ç†"""

    def __init__(self, session_id: str, ttl: int = 604800):
        self.session_id = session_id
        self.key = f"chat:history:{session_id}"
        self.ttl = ttl  # é»˜è®¤7å¤©è¿‡æœŸ

    @property
    def messages(self) -> list[BaseMessage]:
        """è·å–æ‰€æœ‰æ¶ˆæ¯"""
        data = redis_client.client.lrange(self.key, 0, -1)
        messages = []

        for item in reversed(data):  # Redis æ˜¯å€’åºçš„
            msg = json.loads(item)
            if msg["type"] == "human":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["type"] == "ai":
                messages.append(AIMessage(content=msg["content"]))

        return messages

    def add_message(self, message: BaseMessage) -> None:
        """æ·»åŠ æ¶ˆæ¯"""
        msg_data = {
            "type": "human" if isinstance(message, HumanMessage) else "ai",
            "content": message.content
        }

        redis_client.client.lpush(self.key, json.dumps(msg_data))
        redis_client.client.expire(self.key, self.ttl)

    def clear(self) -> None:
        """æ¸…ç©ºæ¶ˆæ¯"""
        redis_client.client.delete(self.key)

# ä½¿ç”¨ç¤ºä¾‹
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    return RedisChatMessageHistory(session_id)

conversational_chain = RunnableWithMessageHistory(
    runnable=chain,
    get_session_history=get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)
```

### 5. ç¼“å­˜è£…é¥°å™¨

```python
from functools import wraps
import hashlib
import json

def cache_result(ttl: int = 3600):
    """Redis ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            key_data = f"{func.__name__}:{str(args)}:{str(kwargs)}"
            cache_key = f"cache:{hashlib.md5(key_data.encode()).hexdigest()}"

            # å°è¯•ä»ç¼“å­˜è·å–
            cached = redis_client.client.get(cache_key)
            if cached:
                return json.loads(cached)

            # æ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs)

            # å­˜å…¥ç¼“å­˜
            redis_client.client.setex(
                cache_key,
                ttl,
                json.dumps(result, ensure_ascii=False)
            )

            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@cache_result(ttl=1800)  # ç¼“å­˜30åˆ†é’Ÿ
async def expensive_operation(param: str):
    # è€—æ—¶æ“ä½œ
    return {"result": f"å¤„ç†ç»“æœ: {param}"}
```

### 6. ç¯å¢ƒå˜é‡é…ç½®

```env
# .env
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=your_password  # å¯é€‰
```

### 7. è¿æ¥æ± é…ç½®ï¼ˆç”Ÿäº§ç¯å¢ƒæ¨èï¼‰

```python
from redis import ConnectionPool

class RedisClient:
    def __init__(self):
        self.pool = ConnectionPool(
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", 6379)),
            db=int(os.getenv("REDIS_DB", 0)),
            password=os.getenv("REDIS_PASSWORD"),
            decode_responses=True,
            max_connections=50  # æœ€å¤§è¿æ¥æ•°
        )
        self.client = redis.Redis(connection_pool=self.pool)
```

### 8. å¸¸è§æ“ä½œé€ŸæŸ¥è¡¨

| æ“ä½œ | å‘½ä»¤ | è¯´æ˜ |
|------|------|------|
| **å­—ç¬¦ä¸²** | `set(key, value)` | è®¾ç½®å€¼ |
| | `get(key)` | è·å–å€¼ |
| | `setex(key, ttl, value)` | è®¾ç½®å€¼+è¿‡æœŸæ—¶é—´ |
| | `delete(key)` | åˆ é™¤ |
| **å“ˆå¸Œ** | `hset(key, field, value)` | è®¾ç½®å­—æ®µ |
| | `hget(key, field)` | è·å–å­—æ®µ |
| | `hgetall(key)` | è·å–æ‰€æœ‰å­—æ®µ |
| | `hdel(key, field)` | åˆ é™¤å­—æ®µ |
| **åˆ—è¡¨** | `lpush(key, value)` | å·¦ä¾§æ’å…¥ |
| | `rpush(key, value)` | å³ä¾§æ’å…¥ |
| | `lrange(key, start, stop)` | è·å–èŒƒå›´ |
| | `ltrim(key, start, stop)` | ä¿ç•™èŒƒå›´ |
| **é€šç”¨** | `expire(key, ttl)` | è®¾ç½®è¿‡æœŸæ—¶é—´ |
| | `ttl(key)` | è·å–å‰©ä½™æ—¶é—´ |
| | `exists(key)` | æ£€æŸ¥æ˜¯å¦å­˜åœ¨ |

### 9. æœ€ä½³å®è·µ

1. **è®¾ç½®è¿‡æœŸæ—¶é—´**ï¼šé¿å…å†…å­˜æ³„æ¼
   ```python
   redis_client.client.setex("key", 3600, "value")  # æ¨è
   redis_client.client.expire("key", 3600)  # ä¹Ÿå¯ä»¥
   ```

2. **ä½¿ç”¨è¿æ¥æ± **ï¼šç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨
   ```python
   pool = ConnectionPool(max_connections=50)
   client = redis.Redis(connection_pool=pool)
   ```

3. **æ‰¹é‡æ“ä½œ**ï¼šä½¿ç”¨ Pipeline æé«˜æ€§èƒ½
   ```python
   pipe = redis_client.client.pipeline()
   for i in range(100):
       pipe.set(f"key:{i}", f"value:{i}")
   pipe.execute()  # ä¸€æ¬¡æ€§æ‰§è¡Œ
   ```

4. **é”™è¯¯å¤„ç†**ï¼š
   ```python
   try:
       value = redis_client.client.get("key")
   except redis.ConnectionError:
       # é™çº§å¤„ç†ï¼šä½¿ç”¨å†…å­˜ç¼“å­˜æˆ–ç›´æ¥è¿”å›
       value = None
   ```

---

## é™„å½•ï¼šFastAPI å¤šç¯å¢ƒé…ç½®æœ€ä½³å®è·µ

> åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œéœ€è¦åŒºåˆ†å¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç¯å¢ƒã€‚ä»¥ä¸‹æ˜¯å¤šç¯å¢ƒé…ç½®çš„å®Œæ•´æ–¹æ¡ˆã€‚

### 1. ç¯å¢ƒæ–‡ä»¶ç»“æ„

```
backend/
â”œâ”€â”€ .env                 # å¼€å‘ç¯å¢ƒï¼ˆé»˜è®¤ï¼Œä¸æäº¤åˆ° Gitï¼‰
â”œâ”€â”€ .env.test            # æµ‹è¯•ç¯å¢ƒï¼ˆä¸æäº¤åˆ° Gitï¼‰
â”œâ”€â”€ .env.production      # ç”Ÿäº§ç¯å¢ƒï¼ˆä¸æäº¤åˆ° Gitï¼‰
â”œâ”€â”€ .env.example         # ç¤ºä¾‹é…ç½®ï¼ˆæäº¤åˆ° Gitï¼‰
â””â”€â”€ app/
    â””â”€â”€ core/
        â””â”€â”€ config.py    # é…ç½®åŠ è½½æ¨¡å—
```

### 2. ç¯å¢ƒé…ç½®æ–‡ä»¶ç¤ºä¾‹

#### `.env.example`ï¼ˆæäº¤åˆ° Gitï¼‰

```env
# ================================
# ç¯å¢ƒé…ç½®ç¤ºä¾‹æ–‡ä»¶
# å¤åˆ¶æ­¤æ–‡ä»¶ä¸º .env å¹¶å¡«å†™çœŸå®å€¼
# ================================

# ç¯å¢ƒæ ‡è¯†
ENVIRONMENT=development
DEBUG=true

# ================================
# API é…ç½®
# ================================
API_V1_STR=/api/v1
PROJECT_NAME="FastAPI AI Agents"
BACKEND_CORS_ORIGINS=["http://localhost:3000","http://localhost:8000"]

# ================================
# LLM é…ç½®
# ================================
LLM_MODEL_ID=gpt-4o
LLM_API_KEY=your_api_key_here
LLM_BASE_URL=https://api.openai.com/v1
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000

# ================================
# Redis é…ç½®
# ================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# ================================
# æ•°æ®åº“é…ç½®
# ================================
DATABASE_URL=postgresql://user:password@localhost:5432/dbname

# ================================
# å®‰å…¨é…ç½®
# ================================
SECRET_KEY=your-secret-key-here-change-in-production
ACCESS_TOKEN_EXPIRE_MINUTES=30

# ================================
# æ—¥å¿—é…ç½®
# ================================
LOG_LEVEL=INFO
```

#### `.env`ï¼ˆå¼€å‘ç¯å¢ƒï¼‰

```env
# å¼€å‘ç¯å¢ƒé…ç½®
ENVIRONMENT=development
DEBUG=true

# LLM - å¼€å‘ç¯å¢ƒ
LLM_MODEL_ID=gpt-4o
LLM_API_KEY=sk-dev-xxx
LLM_BASE_URL=https://api.openai.com/v1
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000

# Redis - æœ¬åœ°
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# æ—¥å¿— - è¯¦ç»†è¾“å‡º
LOG_LEVEL=DEBUG
```

#### `.env.test`ï¼ˆæµ‹è¯•ç¯å¢ƒï¼‰

```env
# æµ‹è¯•ç¯å¢ƒé…ç½®
ENVIRONMENT=test
DEBUG=false

# LLM - æµ‹è¯•ç¯å¢ƒï¼ˆå¯ä»¥ä½¿ç”¨ Mock æˆ–æ›´ä¾¿å®œçš„æ¨¡å‹ï¼‰
LLM_MODEL_ID=gpt-3.5-turbo
LLM_API_KEY=sk-test-xxx
LLM_BASE_URL=https://api.openai.com/v1
LLM_TEMPERATURE=0.5
LLM_MAX_TOKENS=1000

# Redis - æµ‹è¯•æœåŠ¡å™¨
REDIS_HOST=redis.test.local
REDIS_PORT=6379
REDIS_DB=1  # ä½¿ç”¨ä¸åŒçš„ DB

# æ•°æ®åº“ - æµ‹è¯•åº“
DATABASE_URL=postgresql://test_user:test_pass@test-db:5432/test_db

# æ—¥å¿—
LOG_LEVEL=WARNING
```

#### `.env.production`ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

```env
# ç”Ÿäº§ç¯å¢ƒé…ç½®
ENVIRONMENT=production
DEBUG=false

# LLM - ç”Ÿäº§ç¯å¢ƒ
LLM_MODEL_ID=gpt-4o
LLM_API_KEY=sk-prod-xxx  # ä½¿ç”¨ç”Ÿäº§ç¯å¢ƒä¸“ç”¨å¯†é’¥
LLM_BASE_URL=https://api.openai.com/v1
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=4000

# Redis - ç”Ÿäº§é›†ç¾¤
REDIS_HOST=redis.production.internal
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=your-strong-password-here
REDIS_SSL=true
REDIS_MAX_CONNECTIONS=50

# æ•°æ®åº“ - ç”Ÿäº§åº“ï¼ˆä½¿ç”¨è¿æ¥æ± ï¼‰
DATABASE_URL=postgresql://prod_user:strong_pass@prod-db:5432/prod_db?pool_size=20&max_overflow=10

# å®‰å…¨ - ç”Ÿäº§ç¯å¢ƒå¯†é’¥ï¼ˆä½¿ç”¨éšæœºç”Ÿæˆçš„å¼ºå¯†é’¥ï¼‰
SECRET_KEY=your-super-secure-random-secret-key-min-32-chars
ACCESS_TOKEN_EXPIRE_MINUTES=60

# CORS - ä»…å…è®¸ç”Ÿäº§åŸŸå
BACKEND_CORS_ORIGINS=["https://your-domain.com"]

# æ—¥å¿—
LOG_LEVEL=ERROR

# ç›‘æ§
SENTRY_DSN=https://xxx@sentry.io/xxx
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=lsv2_xxx
```

### 3. é…ç½®åŠ è½½æ¨¡å—

```python
# app/core/config.py
from pydantic_settings import BaseSettings
from typing import List
import os

class Settings(BaseSettings):
    """åº”ç”¨é…ç½®ç±»"""

    # ç¯å¢ƒæ ‡è¯†
    ENVIRONMENT: str = "development"
    DEBUG: bool = True

    # API é…ç½®
    API_V1_STR: str = "/api/v1"
    PROJECT_NAME: str = "FastAPI AI Agents"
    BACKEND_CORS_ORIGINS: List[str] = ["http://localhost:3000"]

    # LLM é…ç½®
    LLM_MODEL_ID: str = "gpt-4o"
    LLM_API_KEY: str
    LLM_BASE_URL: str = "https://api.openai.com/v1"
    LLM_TEMPERATURE: float = 0.7
    LLM_MAX_TOKENS: int = 2000

    # Redis é…ç½®
    REDIS_HOST: str = "localhost"
    REDIS_PORT: int = 6379
    REDIS_DB: int = 0
    REDIS_PASSWORD: str = ""
    REDIS_MAX_CONNECTIONS: int = 50

    # æ•°æ®åº“é…ç½®
    DATABASE_URL: str

    # å®‰å…¨é…ç½®
    SECRET_KEY: str
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30

    # æ—¥å¿—é…ç½®
    LOG_LEVEL: str = "INFO"

    # ç›‘æ§é…ç½®
    SENTRY_DSN: str = ""

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = True

# æ ¹æ®ç¯å¢ƒå˜é‡åŠ è½½ä¸åŒçš„é…ç½®æ–‡ä»¶
def get_settings() -> Settings:
    """è·å–å½“å‰ç¯å¢ƒçš„é…ç½®"""
    env = os.getenv("ENVIRONMENT", "development")

    env_file_map = {
        "development": ".env",
        "test": ".env.test",
        "production": ".env.production",
    }

    env_file = env_file_map.get(env, ".env")

    return Settings(_env_file=env_file)

# å…¨å±€é…ç½®å®ä¾‹
settings = get_settings()
```

### 4. è¿è¡Œå‘½ä»¤

#### å¼€å‘ç¯å¢ƒ

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨ uvï¼ˆæ¨èï¼‰
cd backend
uv run fastapi dev app/main.py

# æ–¹å¼äºŒï¼šæ˜ç¡®æŒ‡å®šç¯å¢ƒ
ENVIRONMENT=development uv run fastapi dev app/main.py

# Windows PowerShell
$ENV:ENVIRONMENT="development"; uv run fastapi dev app/main.py
```

#### æµ‹è¯•ç¯å¢ƒ

```bash
# è¿è¡Œæµ‹è¯•
cd backend
ENVIRONMENT=test uv run pytest

# ä½¿ç”¨æµ‹è¯•ç¯å¢ƒå¯åŠ¨æœåŠ¡
ENVIRONMENT=test uv run fastapi dev app/main.py

# Windows PowerShell
$ENV:ENVIRONMENT="test"; uv run fastapi dev app/main.py
```

#### ç”Ÿäº§ç¯å¢ƒ

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨ uvicornï¼ˆæ¨èï¼‰
cd backend
ENVIRONMENT=production uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4

# æ–¹å¼äºŒï¼šä½¿ç”¨ Docker
docker run -d \
  --name fastapi-app \
  --env-file .env.production \
  -p 8000:8000 \
  your-image-name

# æ–¹å¼ä¸‰ï¼šä½¿ç”¨ systemdï¼ˆLinux æœåŠ¡ï¼‰
sudo systemctl start fastapi
```

### 5. Docker å¤šç¯å¢ƒé…ç½®

#### `Dockerfile`

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY pyproject.toml ./
RUN pip install --no-cache-dir -e .

# å¤åˆ¶ä»£ç 
COPY ./app ./app

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡ï¼‰
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### `docker-compose.yml`

```yaml
version: '3.8'

services:
  # å¼€å‘ç¯å¢ƒ
  app-dev:
    build: .
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - ./app:/app/app  # çƒ­é‡è½½
    environment:
      - ENVIRONMENT=development
    command: uvicorn app.main:app --host 0.0.0.0 --reload

  # æµ‹è¯•ç¯å¢ƒ
  app-test:
    build: .
    ports:
      - "8001:8000"
    env_file:
      - .env.test
    environment:
      - ENVIRONMENT=test
    command: uv run pytest

  # ç”Ÿäº§ç¯å¢ƒ
  app-prod:
    build: .
    ports:
      - "8000:8000"
    env_file:
      - .env.production
    environment:
      - ENVIRONMENT=production
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4

  # Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  redis_data:
```

#### ä½¿ç”¨ Docker Compose

```bash
# å¼€å‘ç¯å¢ƒ
docker-compose up app-dev

# æµ‹è¯•ç¯å¢ƒ
docker-compose up app-test

# ç”Ÿäº§ç¯å¢ƒ
docker-compose up -d app-prod

# åå°è¿è¡Œ
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f app-prod
```

### 6. Git é…ç½®

#### `.gitignore`

```gitignore
# ç¯å¢ƒé…ç½®æ–‡ä»¶
.env
.env.test
.env.production
.env.local

# ä½†ä¿ç•™ç¤ºä¾‹æ–‡ä»¶
!.env.example
```

### 7. ç¯å¢ƒåˆ‡æ¢çš„æœ€ä½³å®è·µ

#### æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬

```bash
#!/bin/bash
# start.sh

ENV=${1:-development}

case $ENV in
  "dev"|"development")
    export ENVIRONMENT=development
    uv run fastapi dev app/main.py
    ;;
  "test")
    export ENVIRONMENT=test
    uv run pytest
    ;;
  "prod"|"production")
    export ENVIRONMENT=production
    uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
    ;;
  *)
    echo "Usage: ./start.sh [dev|test|prod]"
    exit 1
    ;;
esac
```

ä½¿ç”¨ï¼š

```bash
./start.sh dev      # å¼€å‘ç¯å¢ƒ
./start.sh test     # æµ‹è¯•ç¯å¢ƒ
./start.sh prod     # ç”Ÿäº§ç¯å¢ƒ
```

#### æ–¹æ¡ˆäºŒï¼šä½¿ç”¨ Makefile

```makefile
# Makefile
.PHONY: dev test prod

dev:
	ENVIRONMENT=development uv run fastapi dev app/main.py

test:
	ENVIRONMENT=test uv run pytest

prod:
	ENVIRONMENT=production uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4

docker-dev:
	docker-compose up app-dev

docker-test:
	docker-compose up app-test

docker-prod:
	docker-compose up -d app-prod
```

ä½¿ç”¨ï¼š

```bash
make dev          # å¼€å‘ç¯å¢ƒ
make test         # æµ‹è¯•ç¯å¢ƒ
make prod         # ç”Ÿäº§ç¯å¢ƒ
make docker-prod  # Docker ç”Ÿäº§ç¯å¢ƒ
```

### 8. ç¯å¢ƒé…ç½®æ£€æŸ¥æ¸…å•

| æ£€æŸ¥é¡¹ | å¼€å‘ç¯å¢ƒ | æµ‹è¯•ç¯å¢ƒ | ç”Ÿäº§ç¯å¢ƒ |
|--------|----------|----------|----------|
| `DEBUG` | `true` | `false` | `false` |
| `ENVIRONMENT` | `development` | `test` | `production` |
| CORS | å…è®¸æ‰€æœ‰æœ¬åœ° | å…è®¸æµ‹è¯•åŸŸå | ä»…å…è®¸ç”Ÿäº§åŸŸå |
| æ—¥å¿—çº§åˆ« | `DEBUG` | `WARNING` | `ERROR` |
| æ•°æ®åº“ | æœ¬åœ°æˆ–å¼€å‘åº“ | ç‹¬ç«‹æµ‹è¯•åº“ | ç”Ÿäº§ä¸»åº“ + ä»åº“ |
| Redis | æœ¬åœ° | æµ‹è¯•æœåŠ¡å™¨ | ç”Ÿäº§é›†ç¾¤ |
| API å¯†é’¥ | å¼€å‘å¯†é’¥ | æµ‹è¯•å¯†é’¥ | ç”Ÿäº§å¯†é’¥ï¼ˆç‹¬ç«‹ï¼‰ |
| è¿æ¥æ±  | å°ï¼ˆ10ï¼‰ | ä¸­ï¼ˆ20ï¼‰ | å¤§ï¼ˆ50+ï¼‰ |
| Workers | 1ï¼ˆçƒ­é‡è½½ï¼‰ | 2 | 4+ |
| ç›‘æ§ | å¯é€‰ | å¯é€‰ | å¿…é¡»ï¼ˆSentry + LangSmithï¼‰ |

### 9. å®‰å…¨å»ºè®®

1. **æ°¸è¿œä¸è¦æäº¤æ•æ„Ÿä¿¡æ¯åˆ° Git**
   ```bash
   # æ£€æŸ¥æ˜¯å¦å·²æ„å¤–æäº¤
   git log --all --full-history -- "*.env"
   ```

2. **ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡**
   - å¼€å‘/æµ‹è¯•ï¼š`.env` æ–‡ä»¶
   - ç”Ÿäº§ï¼šAWS Secrets Manager / Azure Key Vault / HashiCorp Vault

3. **ç”Ÿäº§ç¯å¢ƒå¼ºåˆ¶æ£€æŸ¥**
   ```python
   # app/main.py
   if settings.ENVIRONMENT == "production" and settings.DEBUG:
       raise ValueError("ç”Ÿäº§ç¯å¢ƒå¿…é¡»å…³é—­ DEBUG æ¨¡å¼ï¼")

   if settings.ENVIRONMENT == "production" and not settings.SECRET_KEY:
       raise ValueError("ç”Ÿäº§ç¯å¢ƒå¿…é¡»è®¾ç½® SECRET_KEYï¼")
   ```

---

## å­¦ä¹ èµ„æº

- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LangGraph æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [FastAPI æ–‡æ¡£](https://fastapi.tiangolo.com/)

---

## æ›´æ–°æ—¥å¿—

- **v1.2.6**: æ›´æ–°è‡³ LangChain 1.2.6 APIï¼Œä½¿ç”¨ LangGraph æ¶æ„
- **v1.0**: é‡‡ç”¨ LCEL è¡¨è¾¾å¼è¯­è¨€
- **v0.1**: åˆå§‹ç‰ˆæœ¬

---

> ğŸ’¡ **æç¤º**: å»ºè®®é…åˆ Swagger UI (`/docs`) è¿›è¡Œæ¥å£æµ‹è¯•ï¼Œå¿«é€Ÿç†è§£æ¯ä¸ªåŠŸèƒ½çš„æ•ˆæœã€‚
